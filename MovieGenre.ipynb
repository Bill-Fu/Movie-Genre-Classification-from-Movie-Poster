{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch, lab_utils, random\n",
    "from torchvision.datasets import CIFAR10 \n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json, string\n",
    "\n",
    "%matplotlib inline\n",
    "Genre_Movies=[]\n",
    "Genres={}\n",
    "my_file=open('input.csv','r')\n",
    "count=0\n",
    "for line in my_file:\n",
    "    ele=line.strip().split(',')\n",
    "    if ele[0] not in Genres:\n",
    "        Genres[ele[0]]=count\n",
    "        count+=1\n",
    "        Genre_Movies.append([])\n",
    "        Genre_Movies[Genres[ele[0]]].append(ele[1])\n",
    "    else:\n",
    "        Genre_Movies[Genres[ele[0]]].append(ele[1])\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_CrossEntropyLoss(object): \n",
    "    # Forward pass -log softmax(input_{label})\n",
    "    def forward(self, inputs, labels):\n",
    "        max_val = inputs.max()  # This is to avoid variable overflows.\n",
    "        exp_inputs = (inputs - max_val).exp()\n",
    "        # This is different than in the previous lab. Avoiding for loops here.\n",
    "        denominators = exp_inputs.sum(1).repeat(inputs.size(1), 1).t()\n",
    "        self.predictions = torch.mul(exp_inputs, 1 / denominators)\n",
    "        # Check what gather does. Just avoiding another for loop.\n",
    "        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()\n",
    "    \n",
    "    # Backward pass \n",
    "    def backward(self, inputs, labels):\n",
    "        grad_inputs = self.predictions.clone()\n",
    "        # Ok, Here we will use a for loop (but it is avoidable too).\n",
    "        for i in range(0, inputs.size(0)):\n",
    "            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1\n",
    "        return grad_inputs \n",
    "\n",
    "# Input: 4 vectors of size 10.\n",
    "testInput = torch.Tensor(4, 10).normal_(0, 0.1)\n",
    "# labels: 4 labels indicating the correct class for each input.\n",
    "labels = torch.LongTensor([3, 4, 4, 8])\n",
    "\n",
    "# Forward and Backward passes:\n",
    "loss_softmax = nn_CrossEntropyLoss()\n",
    "loss = loss_softmax.forward(testInput, labels)\n",
    "gradInputs = loss_softmax.backward(testInput, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_Linear(object):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        self.weight = torch.Tensor(inputSize, outputSize).normal_(0, 0.01)\n",
    "        self.gradWeight = torch.Tensor(inputSize, outputSize)\n",
    "        self.bias = torch.Tensor(outputSize).zero_()\n",
    "        self.gradBias = torch.Tensor(outputSize)\n",
    "    \n",
    "    # Forward pass, inputs is a matrix of size batchSize x inputSize\n",
    "    def forward(self, inputs):\n",
    "        # This one needs no change, it just becomes matrix x matrix multiplication\n",
    "        # as opposed to just vector x matrix multiplication as we had before.\n",
    "        return torch.matmul(inputs, self.weight) + self.bias\n",
    "    \n",
    "    # Backward pass, in addition to compute gradients for the weight and bias.\n",
    "    # It has to compute gradients with respect to inputs. \n",
    "    def backward(self, inputs, gradOutput):\n",
    "        self.gradWeight = torch.matmul(inputs.t(), gradOutput)\n",
    "        self.gradBias = gradOutput.sum(0)\n",
    "        return torch.matmul(gradOutput, self.weight.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_ReLU(object):\n",
    "    # pytorch has an element-wise max function.\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs.clone()\n",
    "        outputs[outputs < 0] = 0\n",
    "        return outputs\n",
    "    \n",
    "    # Make sure the backward pass is absolutely clear.\n",
    "    def backward(self, inputs, gradOutput):\n",
    "        gradInputs = gradOutput.clone()\n",
    "        gradInputs[inputs < 0] = 0\n",
    "        return gradInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(network, criterion, optimizer, trainLoader, valLoader, n_epochs = 10, use_gpu = False):\n",
    "\n",
    "    if use_gpu:\n",
    "        network = network.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    # Training loop.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)\n",
    "        network.train()  # This is important to call before training!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            # Loss is a variable, and calling backward on a Variable will\n",
    "            # compute all the gradients that lead to that Variable taking on its\n",
    "            # current value.\n",
    "            loss.backward() \n",
    "\n",
    "            # Weight and bias updates.\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)\n",
    "        network.eval()  # This is important to call before evaluating!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "       # Convolutional layers.\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 32, 5)\n",
    "        \n",
    "        # Linear layers.\n",
    "        self.fc1 = nn.Linear(32*42*64, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        print out.size()\n",
    "        # This flattens the output of the previous layer into a vector.\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 4 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "import torch.utils.data as data\n",
    "import skimage.transform as pictransform\n",
    "\n",
    "trainLabel=[]\n",
    "valLabel=[]\n",
    "trainImg=[]\n",
    "valImg=[]\n",
    "\n",
    "\n",
    "for i, mvid in enumerate(Genre_Movies[Genres['Horror']]):\n",
    "    img=Image.open('./posters/'+mvid+'.jpg').convert('RGB')\n",
    "    if i<500:\n",
    "        valImg.append(img)\n",
    "        valLabel.append(1)\n",
    "    else:\n",
    "        trainImg.append(img)\n",
    "        trainLabel.append(1)\n",
    "\n",
    "for i, mvid in enumerate(Genre_Movies[Genres['Adventure']]):\n",
    "    img=Image.open('./posters/'+mvid+'.jpg').convert('RGB')\n",
    "    if i<500:\n",
    "        valImg.append(img)\n",
    "        valLabel.append(0)\n",
    "    else:\n",
    "        trainImg.append(img)\n",
    "        trainLabel.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, images, labels,imgTransform):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = imgTransform  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.images[index], self.labels[index]\n",
    "        img=imgTransform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c296dbd60da4b55aa93e51b848e3a87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf8603da40d4d53b024d55cd5bf1257"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b5e2b2402646d9a902d3e52681e5c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e00e19ec84a40258562aea7e003158c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd4407108514b3d984b0fb084bdec75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf091372317a425f9a1eb77cfd337f4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c524c3b9219b45c9af1c2de9d68712e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c720103ce9e4dfba522691d7a1f4e8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b086b7d3e040f6ab64d98a0c435b5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296a058566314cd5b01aeecfea11c06c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7691a802aaa34d43a179f5dfc5c9ada7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c145a36e8d7a474b8701f38f478af894"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1493cf4dce419a8f5f74f4d34d1900"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "imgTransform = transforms.Compose([transforms.Scale((182,268)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                                        (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "mytrainset=MyDataset(trainImg,trainLabel,imgTransform)\n",
    "myvalset=MyDataset(valImg,valLabel,imgTransform)\n",
    "\n",
    "mytrainLoader = torch.utils.data.DataLoader(mytrainset, batch_size = 64, \n",
    "                                          shuffle = True, num_workers = 0)\n",
    "myvalLoader = torch.utils.data.DataLoader(myvalset, batch_size = 64, \n",
    "                                        shuffle = False, num_workers = 0)\n",
    "\n",
    "learningRate = 0.05\n",
    "   \n",
    "        \n",
    "# Definition of our network.\n",
    "network = AlexNet()\n",
    "\n",
    "#Definition of our loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definition of optimization strategy.\n",
    "optimizer = optim.SGD(network.parameters(), lr = learningRate, momentum=0.85)\n",
    "\n",
    "# Train the previously defined model.\n",
    "train_model(network, criterion, optimizer, mytrainLoader, myvalLoader, n_epochs = 20, use_gpu = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
