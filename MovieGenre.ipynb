{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch, lab_utils, random\n",
    "from torchvision.datasets import CIFAR10 \n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json, string\n",
    "import os  \n",
    "import torch.utils.data as data\n",
    "import skimage.transform as pictransform\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_CrossEntropyLoss(object): \n",
    "    # Forward pass -log softmax(input_{label})\n",
    "    def forward(self, inputs, labels):\n",
    "        max_val = inputs.max()  # This is to avoid variable overflows.\n",
    "        exp_inputs = (inputs - max_val).exp()\n",
    "        # This is different than in the previous lab. Avoiding for loops here.\n",
    "        denominators = exp_inputs.sum(1).repeat(inputs.size(1), 1).t()\n",
    "        self.predictions = torch.mul(exp_inputs, 1 / denominators)\n",
    "        # Check what gather does. Just avoiding another for loop.\n",
    "        return -self.predictions.log().gather(1, labels.view(-1, 1)).mean()\n",
    "    \n",
    "    # Backward pass \n",
    "    def backward(self, inputs, labels):\n",
    "        grad_inputs = self.predictions.clone()\n",
    "        # Ok, Here we will use a for loop (but it is avoidable too).\n",
    "        for i in range(0, inputs.size(0)):\n",
    "            grad_inputs[i][labels[i]] = grad_inputs[i][labels[i]] - 1\n",
    "        return grad_inputs \n",
    "\n",
    "# Input: 4 vectors of size 10.\n",
    "testInput = torch.Tensor(4, 10).normal_(0, 0.1)\n",
    "# labels: 4 labels indicating the correct class for each input.\n",
    "labels = torch.LongTensor([3, 4, 4, 8])\n",
    "\n",
    "# Forward and Backward passes:\n",
    "loss_softmax = nn_CrossEntropyLoss()\n",
    "loss = loss_softmax.forward(testInput, labels)\n",
    "gradInputs = loss_softmax.backward(testInput, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_Linear(object):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        self.weight = torch.Tensor(inputSize, outputSize).normal_(0, 0.01)\n",
    "        self.gradWeight = torch.Tensor(inputSize, outputSize)\n",
    "        self.bias = torch.Tensor(outputSize).zero_()\n",
    "        self.gradBias = torch.Tensor(outputSize)\n",
    "    \n",
    "    # Forward pass, inputs is a matrix of size batchSize x inputSize\n",
    "    def forward(self, inputs):\n",
    "        # This one needs no change, it just becomes matrix x matrix multiplication\n",
    "        # as opposed to just vector x matrix multiplication as we had before.\n",
    "        return torch.matmul(inputs, self.weight) + self.bias\n",
    "    \n",
    "    # Backward pass, in addition to compute gradients for the weight and bias.\n",
    "    # It has to compute gradients with respect to inputs. \n",
    "    def backward(self, inputs, gradOutput):\n",
    "        self.gradWeight = torch.matmul(inputs.t(), gradOutput)\n",
    "        self.gradBias = gradOutput.sum(0)\n",
    "        return torch.matmul(gradOutput, self.weight.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_ReLU(object):\n",
    "    # pytorch has an element-wise max function.\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs.clone()\n",
    "        outputs[outputs < 0] = 0\n",
    "        return outputs\n",
    "    \n",
    "    # Make sure the backward pass is absolutely clear.\n",
    "    def backward(self, inputs, gradOutput):\n",
    "        gradInputs = gradOutput.clone()\n",
    "        gradInputs[inputs < 0] = 0\n",
    "        return gradInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(network, criterion, optimizer, trainLoader, valLoader, n_epochs = 10, use_gpu = False):\n",
    "\n",
    "    if use_gpu:\n",
    "        network = network.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    # Training loop.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)\n",
    "        network.train()  # This is important to call before training!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            # Loss is a variable, and calling backward on a Variable will\n",
    "            # compute all the gradients that lead to that Variable taking on its\n",
    "            # current value.\n",
    "            loss.backward() \n",
    "\n",
    "            # Weight and bias updates.\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)\n",
    "        network.eval()  # This is important to call before evaluating!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "       # Convolutional layers.\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 32, 5)\n",
    "        \n",
    "        # Linear layers.\n",
    "        self.fc1 = nn.Linear(32*42*64, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        # This flattens the output of the previous layer into a vector.\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 4 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, images, labels,imgTransform):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = imgTransform  \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.images[index], self.labels[index]\n",
    "        img=imgTransform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Genres contains genre's name and its index\n",
    "#Genres_Movies contains movieids for every genre, the index coprresponds to index in Genres\n",
    "Genre_Movies=[]\n",
    "Genres={}\n",
    "my_file=open('input.csv','r')\n",
    "count=0\n",
    "for line in my_file:\n",
    "    ele=line.strip().split(',')\n",
    "    if ele[0] not in Genres:\n",
    "        Genres[ele[0]]=count\n",
    "        count+=1\n",
    "        Genre_Movies.append([])\n",
    "        Genre_Movies[Genres[ele[0]]].append(ele[1])\n",
    "    else:\n",
    "        Genre_Movies[Genres[ele[0]]].append(ele[1])\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract test dataset\n",
    "Test_Movies = []\n",
    "for i in range(len(Genres)):\n",
    "    length = len(Genre_Movies[i])\n",
    "    Test_Movies.append([])\n",
    "    for j in range(int(length/5)):\n",
    "        a = random.sample(Genre_Movies[i],1)\n",
    "        Genre_Movies[i].remove(a[0])\n",
    "        Test_Movies[i].append(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImgList=[]\n",
    "valImgList=[]\n",
    "trainLabelList=[]\n",
    "valLabelList=[]\n",
    "for index, gen in enumerate(Genres):\n",
    "    if index==1:\n",
    "        trainLabel=[]\n",
    "        valLabel=[]\n",
    "        trainImg=[]\n",
    "        valImg=[]\n",
    "        #Add image with label 1\n",
    "        for i, mvid in enumerate(Genre_Movies[Genres[gen]]):\n",
    "            if i<5500:\n",
    "                img=Image.open('./posters/'+mvid+'.jpg').convert('RGB')\n",
    "                if i<500:\n",
    "                    valImg.append(img)\n",
    "                    valLabel.append(1)\n",
    "                else:\n",
    "                    trainImg.append(img)\n",
    "                    trainLabel.append(1)\n",
    "            else:\n",
    "                break\n",
    "        if len(trainImg)<5000:\n",
    "            temp=random.sample(Genre_Movies[Genres[gen]],5000-len(trainImg))\n",
    "            for mvid in temp:\n",
    "                img=Image.open('./posters/'+mvid+'.jpg').convert('RGB')\n",
    "                trainImg.append(img)\n",
    "        while(len(trainLabel)<5000): trainLabel.append(1)\n",
    "        #Add image with label 0\n",
    "        label0Img=[]\n",
    "        for i in Genre_Movies:\n",
    "            if i!=Genre_Movies[Genres[gen]]:\n",
    "                for j in i:\n",
    "                    if j not in Genre_Movies[Genres[gen]]:\n",
    "                        label0Img.append(j)\n",
    "        samples=random.sample(label0Img,5500)\n",
    "        for i, mvid in enumerate(samples):\n",
    "            img=Image.open('./posters/'+mvid+'.jpg').convert('RGB')\n",
    "            if i<500:\n",
    "                valImg.append(img)\n",
    "                valLabel.append(0)\n",
    "            else:\n",
    "                trainImg.append(img)\n",
    "                trainLabel.append(0)        \n",
    "        trainImgList.append(trainImg)\n",
    "        valImgList.append(valImg)\n",
    "        trainLabelList.append(trainLabel)\n",
    "        valLabelList.append(valLabel)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "learningRate = 0.03\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "imgTransform = transforms.Compose([transforms.Scale((182,268)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                                        (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "#classifier for 9 genres\n",
    "networks=[]\n",
    "for i in range(len(Genres)):\n",
    "    network = LeNet()\n",
    "    optimizer = optim.SGD(network.parameters(), lr = learningRate, momentum=0.85)\n",
    "    mytrainset=MyDataset(trainImgList[i],trainLabelList[i],imgTransform)\n",
    "    myvalset=MyDataset(valImgList[i],valLabelList[i],imgTransform)\n",
    "\n",
    "    mytrainLoader = torch.utils.data.DataLoader(mytrainset, batch_size = 64, \n",
    "                                          shuffle = True, num_workers = 0)\n",
    "    myvalLoader = torch.utils.data.DataLoader(myvalset, batch_size = 64, \n",
    "                                        shuffle = False, num_workers = 0)\n",
    "\n",
    "    train_model(network, criterion, optimizer, mytrainLoader, myvalLoader, n_epochs = 1, use_gpu = False)\n",
    "    networks.append(network)\n",
    "    print \"network %d is done\"%(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test network\n",
    "Test_Sum=0.0\n",
    "Correct_Sum=0.0\n",
    "preprocessFn = transforms.Compose([transforms.ToTensor(), \n",
    "                                   transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                                        std=[0.229, 0.224, 0.225])])\n",
    "for i in range(len(Genres)):\n",
    "    Test_dic={}\n",
    "    for mvid in Test_Movies[i]:\n",
    "        Test_dic[mvid]=1\n",
    "    for index, l in enumerate(Test_Movies):\n",
    "        if index!=i:\n",
    "            for mvid in l: \n",
    "                if mvid not in Test_dic:\n",
    "                    Test_dic[mvid]=0\n",
    "    print len(Test_dic)\n",
    "    networks[i].eval()  \n",
    "    for index, l in enumerate(Test_Movies):\n",
    "        for mvid in l:\n",
    "            image = Image.open('./posters/'+mvid+'.jpg').convert('RGB') \n",
    "            inputVar =  Variable(preprocessFn(image).unsqueeze(0))\n",
    "            Predictions = F.softmax(network(inputVar))\n",
    "            Predictions = Predictions.data\n",
    "            max_score, max_label = Predictions.max(1)\n",
    "            if max_label.numpy()==Test_dic[mvid]:\n",
    "                Correct_Sum+=1\n",
    "            Test_Sum+=1\n",
    "            break\n",
    "    print \"The accuracy of classifier %d is %f\"%(i,Correct_Sum/Test_Sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
